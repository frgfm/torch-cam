{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TorchCAM: class activation explorer","text":"<p>TorchCAM provides a minimal yet flexible way to explore the spatial importance of features on your PyTorch model outputs. Check out the live demo on HuggingFace Spaces \ud83e\udd17</p> <p> </p> <p> Source: image from woopets (activation maps created with a pretrained Resnet-18) </p> <p>This project is meant for:</p> <ul> <li>\u26a1 exploration: easily assess the influence of spatial features on your model's outputs</li> <li>\ud83d\udc69\u200d\ud83d\udd2c research: quickly implement your own ideas for new CAM methods</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Create and activate a virtual environment and then install TorchCAM:</p> <pre><code>pip install torchcam\n</code></pre> <p>Check out the installation guide for more options</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>Get an image and a model:</p> <pre><code>from torchvision.io import decode_image\nfrom torchvision.models import get_model, get_model_weights\n\nweights = get_model_weights(\"resnet18\").DEFAULT\nmodel = get_model(\"resnet18\", weights=weights).eval()\npreprocess = weights.transforms()\n\nimg_path = \"path/to/your/image.jpg\"\n\nimg = decode_image(img_path)\ninput_tensor = preprocess(img)\n</code></pre> <p>Compute the class activation map:</p> <pre><code>from torchcam.methods import LayerCAM\n\nwith LayerCAM(model) as cam_extractor:\n  out = model(input_tensor.unsqueeze(0))\n  # Retrieve the CAM by passing the class index and the model output\n  activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n</code></pre> <p>Display it:</p> <pre><code>import matplotlib.pyplot as plt\nfrom torchvision.transforms.v2.functional import to_pil_image\nfrom torchcam.utils import overlay_mask\n\n# Resize the CAM and overlay it\nresult = overlay_mask(to_pil_image(img), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\nplt.imshow(result); plt.axis('off'); plt.tight_layout(); plt.show()\n</code></pre> <p></p>"},{"location":"#cam-zoo","title":"CAM zoo","text":""},{"location":"#activation-based-methods","title":"Activation-based methods","text":"<ul> <li>CAM from \"Learning Deep Features for Discriminative Localization\"</li> <li>Score-CAM from \"Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks\"</li> <li>SS-CAM from \"SS-CAM: Smoothed Score-CAM for Sharper Visual Feature Localization\"</li> <li>IS-CAM from \"IS-CAM: Integrated Score-CAM for axiomatic-based explanations\"</li> </ul>"},{"location":"#gradient-based-methods","title":"Gradient-based methods","text":"<ul> <li>Grad-CAM from \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\"</li> <li>Grad-CAM++ from \"Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks\"</li> <li>Smooth Grad-CAM++ from \"Smooth Grad-CAM++: An Enhanced Inference Level Visualization Technique for Deep Convolutional Neural Network Models\"</li> <li>X-Grad-CAM from \"Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs\"</li> <li>Layer-CAM from \"LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\"</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#virtual-environment","title":"Virtual environment","text":"<p>Tip</p> <p>You will need an environment manager, and I cannot recommend enough uv.</p> <p>Create a virtual environment with your prefered Python version (3.11 or higher is required to use TorchCAM): <pre><code>$ uv venv --python 3.11\n</code></pre></p> StableLatest <pre><code>$ uv pip install torchcam\n</code></pre> <pre><code>$ uv pip install torch-cam @ git+https://github.com/frgfm/torch-cam.git\n</code></pre>"},{"location":"getting-started/installation/#system-installation","title":"System installation","text":"<p>You'll need Python 3.11 or higher, and a package installer like uv or pip.</p> StableLatestStable (pip)Latest (pip) <pre><code>$ uv pip install --system torchcam\n</code></pre> <pre><code>$ uv pip install --system torch-cam @ git+https://github.com/frgfm/torch-cam.git\n</code></pre> <pre><code>$ pip install torchcam\n</code></pre> <pre><code>$ pip install torch-cam @ git+https://github.com/frgfm/torch-cam.git\n</code></pre> <p>Info</p> <p>TorchCAM is built on top of PyTorch which is a complex dependency. Proper installation depends on your system and available hardware. You can refer to installation guide of uv which is quite detailed.</p>"},{"location":"getting-started/notebooks/","title":"TorchCAM Notebooks","text":"<p>Here are some notebooks compiled for users to better leverage the library capabilities:</p> Notebook Description Quicktour A presentation of the main features of TorchCAM Latency benchmark How to benchmark the latency of a CAM method Performance benchmark How to benchmark the interpretability of a CAM method"},{"location":"notes/changelog/","title":"Changelog","text":""},{"location":"notes/changelog/#v021-2022-07-16","title":"v0.2.1 (2022-07-16)","text":"<p>Release note: v0.2.1</p>"},{"location":"notes/changelog/#v020-2022-02-05","title":"v0.2.0 (2022-02-05)","text":"<p>Release note: v0.2.0</p>"},{"location":"notes/changelog/#v013-2020-10-27","title":"v0.1.3 (2020-10-27)","text":"<p>Release note: v0.1.3</p>"},{"location":"notes/changelog/#v012-2020-06-21","title":"v0.1.2 (2020-06-21)","text":"<p>Release note: v0.1.2</p>"},{"location":"notes/changelog/#v011-2020-05-12","title":"v0.1.1 (2020-05-12)","text":"<p>Release note: v0.1.1</p>"},{"location":"notes/changelog/#v010-2020-05-11","title":"v0.1.0 (2020-05-11)","text":"<p>Release note: v0.1.0</p>"},{"location":"reference/methods/","title":"Interpretability methods","text":""},{"location":"reference/methods/#class-activation-map","title":"Class activation map","text":"<p>The class activation map gives you the importance of each region of a feature map on a model's output. More specifically, a class activation map is relative to:</p> <ul> <li>the layer at which it is computed (e.g. the N-th layer of your model)</li> <li>the model's classification output (e.g. the raw logits of the model)</li> <li>the class index to focus on</li> </ul> <p>With TorchCAM, the target layer is selected when you create your CAM extractor. You will need to pass the model logits to the extractor and a class index for it to do its magic!</p>"},{"location":"reference/methods/#activation-based-methods","title":"Activation-based methods","text":"<p>Methods related to activation-based class activation maps.</p>"},{"location":"reference/methods/#torchcam.methods.CAM","title":"CAM","text":"<pre><code>CAM(model: Module, target_layer: Module | str | list[Module | str] | None = None, fc_layer: Module | str | None = None, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"Learning Deep Features for Discriminative Localization\".</p> <p>The Class Activation Map (CAM) is defined for image classification models that have global pooling at the end of the visual feature extraction block. The localization map is computed as follows:</p> \\[ L^{(c)}_{CAM}(x, y) = ReLU\\Big(\\sum\\limits_k w_k^{(c)} A_k(x, y)\\Big) \\] <p>where \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), and \\(w_k^{(c)}\\) is the weight corresponding to class \\(c\\) for unit \\(k\\) in the fully connected layer.</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import CAM\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith CAM(model, 'layer4', 'fc') as cam_extractor:\n    with torch.inference_mode(): out = model(input_tensor)\n    cam = cam_extractor(class_idx=100)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>fc_layer</code> <p>either the fully connected layer itself or its name</p> <p> TYPE: <code>Module | str | None</code> DEFAULT: <code>None</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the argument is invalid</p> <code>TypeError</code> <p>if the argument type is invalid</p> Source code in <code>torchcam/methods/activation.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    fc_layer: nn.Module | str | None = None,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    if isinstance(target_layer, list) and len(target_layer) &gt; 1:\n        raise ValueError(\"base CAM does not support multiple target layers\")\n\n    super().__init__(model, target_layer, input_shape, **kwargs)\n\n    if isinstance(fc_layer, str):\n        fc_name = fc_layer\n    # Find the location of the module\n    elif isinstance(fc_layer, nn.Module):\n        fc_name = self._resolve_layer_name(fc_layer)\n    # If the layer is not specified, try automatic resolution\n    elif fc_layer is None:\n        lin_layers = [layer_name for layer_name, m in model.named_modules() if isinstance(m, nn.Linear)]\n        # Warn the user of the choice\n        if len(lin_layers) == 0:\n            raise ValueError(\"unable to resolve `fc_layer` automatically, please specify its value.\")\n        if len(lin_layers) &gt; 1:\n            raise ValueError(\"This CAM method does not support multiple fully connected layers.\")\n        fc_name = lin_layers[0]\n        logger.warning(f\"no value was provided for `fc_layer`, thus set to '{fc_name}'.\")\n    else:\n        raise TypeError(\"invalid argument type for `fc_layer`\")\n    # Softmax weight\n    self._fc_weights = self.submodule_dict[fc_name].weight.data\n    # squeeze to accomodate replacement by Conv1x1\n    if self._fc_weights.ndim &gt; 2:\n        self._fc_weights = self._fc_weights.view(*self._fc_weights.shape[:2])\n</code></pre>"},{"location":"reference/methods/#torchcam.methods.CAM","title":"torchcam.methods.CAM","text":"<pre><code>CAM(model: Module, target_layer: Module | str | list[Module | str] | None = None, fc_layer: Module | str | None = None, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"Learning Deep Features for Discriminative Localization\".</p> <p>The Class Activation Map (CAM) is defined for image classification models that have global pooling at the end of the visual feature extraction block. The localization map is computed as follows:</p> \\[ L^{(c)}_{CAM}(x, y) = ReLU\\Big(\\sum\\limits_k w_k^{(c)} A_k(x, y)\\Big) \\] <p>where \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), and \\(w_k^{(c)}\\) is the weight corresponding to class \\(c\\) for unit \\(k\\) in the fully connected layer.</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import CAM\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith CAM(model, 'layer4', 'fc') as cam_extractor:\n    with torch.inference_mode(): out = model(input_tensor)\n    cam = cam_extractor(class_idx=100)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>fc_layer</code> <p>either the fully connected layer itself or its name</p> <p> TYPE: <code>Module | str | None</code> DEFAULT: <code>None</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the argument is invalid</p> <code>TypeError</code> <p>if the argument type is invalid</p> Source code in <code>torchcam/methods/activation.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    fc_layer: nn.Module | str | None = None,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    if isinstance(target_layer, list) and len(target_layer) &gt; 1:\n        raise ValueError(\"base CAM does not support multiple target layers\")\n\n    super().__init__(model, target_layer, input_shape, **kwargs)\n\n    if isinstance(fc_layer, str):\n        fc_name = fc_layer\n    # Find the location of the module\n    elif isinstance(fc_layer, nn.Module):\n        fc_name = self._resolve_layer_name(fc_layer)\n    # If the layer is not specified, try automatic resolution\n    elif fc_layer is None:\n        lin_layers = [layer_name for layer_name, m in model.named_modules() if isinstance(m, nn.Linear)]\n        # Warn the user of the choice\n        if len(lin_layers) == 0:\n            raise ValueError(\"unable to resolve `fc_layer` automatically, please specify its value.\")\n        if len(lin_layers) &gt; 1:\n            raise ValueError(\"This CAM method does not support multiple fully connected layers.\")\n        fc_name = lin_layers[0]\n        logger.warning(f\"no value was provided for `fc_layer`, thus set to '{fc_name}'.\")\n    else:\n        raise TypeError(\"invalid argument type for `fc_layer`\")\n    # Softmax weight\n    self._fc_weights = self.submodule_dict[fc_name].weight.data\n    # squeeze to accomodate replacement by Conv1x1\n    if self._fc_weights.ndim &gt; 2:\n        self._fc_weights = self._fc_weights.view(*self._fc_weights.shape[:2])\n</code></pre>"},{"location":"reference/methods/#torchcam.methods.ScoreCAM","title":"torchcam.methods.ScoreCAM","text":"<pre><code>ScoreCAM(model: Module, target_layer: Module | str | list[Module | str] | None = None, batch_size: int = 32, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks\".</p> <p>The localization map is computed as follows:</p> \\[ L^{(c)}_{Score-CAM}(x, y) = ReLU\\Big(\\sum\\limits_k w_k^{(c)} A_k(x, y)\\Big) \\] <p>with the coefficient \\(w_k^{(c)}\\) being defined as:</p> \\[ w_k^{(c)} = softmax\\Big(Y^{(c)}(M_k) - Y^{(c)}(X_b)\\Big)_k \\] <p>where \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), \\(Y^{(c)}(X)\\) is the model output score for class \\(c\\) before softmax for input \\(X\\), \\(X_b\\) is a baseline image, and \\(M_k\\) is defined as follows:</p> \\[ M_k = \\frac{U(A_k) - \\min\\limits_m U(A_m)}{\\max\\limits_m  U(A_m) - \\min\\limits_m  U(A_m)}) \\odot X_b \\] <p>where \\(\\odot\\) refers to the element-wise multiplication and \\(U\\) is the upsampling operation.</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import ScoreCAM\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith ScoreCAM(model, 'layer4') as cam_extractor:\n    with torch.inference_mode(): out = model(input_tensor)\n    cam = cam_extractor(class_idx=100)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>batch size used to forward masked inputs</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> Source code in <code>torchcam/methods/activation.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    batch_size: int = 32,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(model, target_layer, input_shape, **kwargs)\n\n    # Input hook\n    self.hook_handles.append(model.register_forward_pre_hook(self._store_input))  # type: ignore[arg-type]\n    self.bs = batch_size\n    # Ensure ReLU is applied to CAM before normalization\n    self._relu = True\n</code></pre>"},{"location":"reference/methods/#torchcam.methods.SSCAM","title":"torchcam.methods.SSCAM","text":"<pre><code>SSCAM(model: Module, target_layer: Module | str | list[Module | str] | None = None, batch_size: int = 32, num_samples: int = 35, std: float = 2.0, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"SS-CAM: Smoothed Score-CAM for Sharper Visual Feature Localization\".</p> <p>The localization map is computed as follows:</p> \\[ L^{(c)}_{SS-CAM}(x, y) = ReLU\\Big(\\sum\\limits_k w_k^{(c)} A_k(x, y)\\Big) \\] <p>with the coefficient \\(w_k^{(c)}\\) being defined as:</p> \\[ w_k^{(c)} = softmax\\Big(\\frac{1}{N} \\sum\\limits_{i=1}^N (Y^{(c)}(\\hat{M_k}) - Y^{(c)}(X_b))\\Big)_k \\] <p>where \\(N\\) is the number of samples used to smooth the weights, \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), \\(Y^{(c)}(X)\\) is the model output score for class \\(c\\) before softmax for input \\(X\\), \\(X_b\\) is a baseline image, and \\(M_k\\) is defined as follows:</p> \\[ \\hat{M_k} = \\Bigg(\\frac{U(A_k) - \\min\\limits_m U(A_m)}{\\max\\limits_m  U(A_m) - \\min\\limits_m  U(A_m)} + \\delta\\Bigg) \\odot X_b \\] <p>where \\(\\odot\\) refers to the element-wise multiplication, \\(U\\) is the upsampling operation, \\(\\delta \\sim \\mathcal{N}(0, \\sigma^2)\\) is the random noise that follows a 0-mean gaussian distribution with a standard deviation of \\(\\sigma\\).</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import SSCAM\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith SSCAM(model, 'layer4') as cam_extractor:\n    with torch.inference_mode(): out = model(input_tensor)\n    cam = cam_extractor(class_idx=100)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>batch size used to forward masked inputs</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>num_samples</code> <p>number of noisy samples used for weight computation</p> <p> TYPE: <code>int</code> DEFAULT: <code>35</code> </p> <code>std</code> <p>standard deviation of the noise added to the normalized activation</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> Source code in <code>torchcam/methods/activation.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    batch_size: int = 32,\n    num_samples: int = 35,\n    std: float = 2.0,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(model, target_layer, batch_size, input_shape, **kwargs)\n\n    self.num_samples = num_samples\n    self.std = std\n    self._distrib = torch.distributions.normal.Normal(0, self.std)  # ty: ignore[unresolved-attribute]\n</code></pre>"},{"location":"reference/methods/#torchcam.methods.ISCAM","title":"torchcam.methods.ISCAM","text":"<pre><code>ISCAM(model: Module, target_layer: Module | str | list[Module | str] | None = None, batch_size: int = 32, num_samples: int = 10, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"IS-CAM: Integrated Score-CAM for axiomatic-based explanations\".</p> <p>The localization map is computed as follows:</p> \\[ L^{(c)}_{ISS-CAM}(x, y) = ReLU\\Big(\\sum\\limits_k w_k^{(c)} A_k(x, y)\\Big) \\] <p>with the coefficient \\(w_k^{(c)}\\) being defined as:</p> \\[ w_k^{(c)} = softmax\\Bigg(\\frac{1}{N} \\sum\\limits_{i=1}^N \\Big(Y^{(c)}(M_i) - Y^{(c)}(X_b)\\Big)\\Bigg)_k \\] <p>where \\(N\\) is the number of samples used to smooth the weights, \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), \\(Y^{(c)}(X)\\) is the model output score for class \\(c\\) before softmax for input \\(X\\), \\(X_b\\) is a baseline image, and \\(M_i\\) is defined as follows:</p> \\[ M_i = \\sum\\limits_{j=0}^{i-1} \\frac{j}{N} \\frac{U(A_k) - \\min\\limits_m U(A_m)}{\\max\\limits_m  U(A_m) - \\min\\limits_m  U(A_m)} \\odot X_b \\] <p>where \\(\\odot\\) refers to the element-wise multiplication, \\(U\\) is the upsampling operation.</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import ISCAM\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith ISCAM(model, 'layer4') as cam_extractor:\n    with torch.inference_mode(): out = model(input_tensor)\n    cam = cam_extractor(class_idx=100)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>batch size used to forward masked inputs</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>num_samples</code> <p>number of noisy samples used for weight computation</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> Source code in <code>torchcam/methods/activation.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    batch_size: int = 32,\n    num_samples: int = 10,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(model, target_layer, batch_size, input_shape, **kwargs)\n\n    self.num_samples = num_samples\n</code></pre>"},{"location":"reference/methods/#gradient-based-methods","title":"Gradient-based methods","text":"<p>Methods related to gradient-based class activation maps.</p>"},{"location":"reference/methods/#torchcam.methods.GradCAM","title":"torchcam.methods.GradCAM","text":"<pre><code>GradCAM(model: Module, target_layer: Module | str | list[Module | str] | None = None, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\".</p> <p>The localization map is computed as follows:</p> \\[ L^{(c)}_{Grad-CAM}(x, y) = ReLU\\Big(\\sum\\limits_k w_k^{(c)} A_k(x, y)\\Big) \\] <p>with the coefficient \\(w_k^{(c)}\\) being defined as:</p> \\[ w_k^{(c)} = \\frac{1}{H \\cdot W} \\sum\\limits_{i=1}^H \\sum\\limits_{j=1}^W \\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)} \\] <p>where \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), and \\(Y^{(c)}\\) is the model output score for class \\(c\\) before softmax.</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import GradCAM\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith GradCAM(model, 'layer4') as cam_extractor:\n    scores = model(input_tensor)\n    cam = cam_extractor(class_idx=100, scores=scores)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> Source code in <code>torchcam/methods/gradient.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(model, target_layer, input_shape, **kwargs)\n    # Ensure ReLU is applied before normalization\n    self._relu = True\n    # Model output is used by the extractor\n    self._score_used = True\n    for idx, name in enumerate(self.target_names):\n        # Trick to avoid issues with inplace operations cf. https://github.com/pytorch/pytorch/issues/61519\n        self.hook_handles.append(self.submodule_dict[name].register_forward_hook(partial(self._hook_g, idx=idx)))\n</code></pre>"},{"location":"reference/methods/#torchcam.methods.GradCAMpp","title":"torchcam.methods.GradCAMpp","text":"<pre><code>GradCAMpp(model: Module, target_layer: Module | str | list[Module | str] | None = None, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks\".</p> <p>The localization map is computed as follows:</p> \\[ L^{(c)}_{Grad-CAM++}(x, y) = \\sum\\limits_k w_k^{(c)} A_k(x, y) \\] <p>with the coefficient \\(w_k^{(c)}\\) being defined as:</p> \\[ w_k^{(c)} = \\sum\\limits_{i=1}^H \\sum\\limits_{j=1}^W \\alpha_k^{(c)}(i, j) \\cdot ReLU\\Big(\\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)}\\Big) \\] <p>where \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), \\(Y^{(c)}\\) is the model output score for class \\(c\\) before softmax, and \\(\\alpha_k^{(c)}(i, j)\\) being defined as:</p> \\[ \\alpha_k^{(c)}(i, j) = \\frac{1}{\\sum\\limits_{i, j} \\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)}} = \\frac{\\frac{\\partial^2 Y^{(c)}}{(\\partial A_k(i,j))^2}}{2 \\cdot \\frac{\\partial^2 Y^{(c)}}{(\\partial A_k(i,j))^2} + \\sum\\limits_{a,b} A_k (a,b) \\cdot \\frac{\\partial^3 Y^{(c)}}{(\\partial A_k(i,j))^3}} \\] <p>if \\(\\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)} = 1\\) else \\(0\\).</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import GradCAMpp\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith GradCAMpp(model, 'layer4') as cam_extractor:\n    scores = model(input_tensor)\n    cam = cam_extractor(class_idx=100, scores=scores)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> Source code in <code>torchcam/methods/gradient.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(model, target_layer, input_shape, **kwargs)\n    # Ensure ReLU is applied before normalization\n    self._relu = True\n    # Model output is used by the extractor\n    self._score_used = True\n    for idx, name in enumerate(self.target_names):\n        # Trick to avoid issues with inplace operations cf. https://github.com/pytorch/pytorch/issues/61519\n        self.hook_handles.append(self.submodule_dict[name].register_forward_hook(partial(self._hook_g, idx=idx)))\n</code></pre>"},{"location":"reference/methods/#torchcam.methods.SmoothGradCAMpp","title":"torchcam.methods.SmoothGradCAMpp","text":"<pre><code>SmoothGradCAMpp(model: Module, target_layer: Module | str | list[Module | str] | None = None, num_samples: int = 4, std: float = 0.3, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"Smooth Grad-CAM++: An Enhanced Inference Level Visualization Technique for Deep Convolutional Neural Network Models\" with a personal correction to the paper (alpha coefficient numerator).</p> <p>The localization map is computed as follows:</p> \\[ L^{(c)}_{Smooth Grad-CAM++}(x, y) = \\sum\\limits_k w_k^{(c)} A_k(x, y) \\] <p>with the coefficient \\(w_k^{(c)}\\) being defined as:</p> \\[ w_k^{(c)} = \\sum\\limits_{i=1}^H \\sum\\limits_{j=1}^W \\alpha_k^{(c)}(i, j) \\cdot ReLU\\Big(\\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)}\\Big) \\] <p>where \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), \\(Y^{(c)}\\) is the model output score for class \\(c\\) before softmax, and \\(\\alpha_k^{(c)}(i, j)\\) being defined as:</p> \\[ \\alpha_k^{(c)}(i, j) = \\frac{\\frac{\\partial^2 Y^{(c)}}{(\\partial A_k(i,j))^2}}{2 \\cdot \\frac{\\partial^2 Y^{(c)}}{(\\partial A_k(i,j))^2} + \\sum\\limits_{a,b} A_k (a,b) \\cdot \\frac{\\partial^3 Y^{(c)}}{(\\partial A_k(i,j))^3}} = \\frac{\\frac{1}{n} \\sum\\limits_{m=1}^n D^{(c, 2)}_k(i, j)}{ \\frac{2}{n} \\sum\\limits_{m=1}^n D^{(c, 2)}_k(i, j) + \\sum\\limits_{a,b} A_k (a,b) \\cdot \\frac{1}{n} \\sum\\limits_{m=1}^n D^{(c, 3)}_k(i, j)} \\] <p>if \\(\\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)} = 1\\) else \\(0\\). Here \\(D^{(c, p)}_k(i, j)\\) refers to the p-th partial derivative of the class score of class \\(c\\) relatively to the activation in layer \\(k\\) at position \\((i, j)\\), and \\(n\\) is the number of samples used to get the gradient estimate.</p> <p>Please note the difference in the numerator of \\(\\alpha_k^{(c)}(i, j)\\), which is actually \\(\\frac{1}{n} \\sum\\limits_{k=1}^n D^{(c, 1)}_k(i,j)\\) in the paper.</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import SmoothGradCAMpp\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith SmoothGradCAMpp(model, 'layer4') as cam_extractor:\n    scores = model(input_tensor)\n    cam = cam_extractor(class_idx=100)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>num_samples</code> <p>number of samples to use for smoothing</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>std</code> <p>standard deviation of the noise</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> Source code in <code>torchcam/methods/gradient.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    num_samples: int = 4,\n    std: float = 0.3,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(model, target_layer, input_shape, **kwargs)\n    # Model scores is not used by the extractor\n    self._score_used = False\n\n    # Input hook\n    self.hook_handles.append(model.register_forward_pre_hook(self._store_input))  # type: ignore[arg-type]\n    # Noise distribution\n    self.num_samples = num_samples\n    self.std = std\n    self._distrib = torch.distributions.normal.Normal(0, self.std)  # ty: ignore[unresolved-attribute]\n    # Specific input hook updater\n    self._ihook_enabled = True\n</code></pre>"},{"location":"reference/methods/#torchcam.methods.XGradCAM","title":"torchcam.methods.XGradCAM","text":"<pre><code>XGradCAM(model: Module, target_layer: Module | str | list[Module | str] | None = None, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs\".</p> <p>The localization map is computed as follows:</p> \\[ L^{(c)}_{XGrad-CAM}(x, y) = ReLU\\Big(\\sum\\limits_k w_k^{(c)} A_k(x, y)\\Big) \\] <p>with the coefficient \\(w_k^{(c)}\\) being defined as:</p> \\[ w_k^{(c)} = \\sum\\limits_{i=1}^H \\sum\\limits_{j=1}^W \\Big( \\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)} \\cdot \\frac{A_k(i, j)}{\\sum\\limits_{m=1}^H \\sum\\limits_{n=1}^W A_k(m, n)} \\Big) \\] <p>where \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), and \\(Y^{(c)}\\) is the model output score for class \\(c\\) before softmax.</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import XGradCAM\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith XGradCAM(model, 'layer4') as cam_extractor:\n    scores = model(input_tensor)\n    cam = cam_extractor(class_idx=100, scores=scores)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> Source code in <code>torchcam/methods/gradient.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(model, target_layer, input_shape, **kwargs)\n    # Ensure ReLU is applied before normalization\n    self._relu = True\n    # Model output is used by the extractor\n    self._score_used = True\n    for idx, name in enumerate(self.target_names):\n        # Trick to avoid issues with inplace operations cf. https://github.com/pytorch/pytorch/issues/61519\n        self.hook_handles.append(self.submodule_dict[name].register_forward_hook(partial(self._hook_g, idx=idx)))\n</code></pre>"},{"location":"reference/methods/#torchcam.methods.LayerCAM","title":"torchcam.methods.LayerCAM","text":"<pre><code>LayerCAM(model: Module, target_layer: Module | str | list[Module | str] | None = None, input_shape: tuple[int, ...] = (3, 224, 224), **kwargs: Any)\n</code></pre> <p>Implements a class activation map extractor as described in \"LayerCAM: Exploring Hierarchical Class Activation Maps for Localization\".</p> <p>The localization map is computed as follows:</p> \\[ L^{(c)}_{Layer-CAM}(x, y) = ReLU\\Big(\\sum\\limits_k w_k^{(c)}(x, y) \\cdot A_k(x, y)\\Big) \\] <p>with the coefficient \\(w_k^{(c)}(x, y)\\) being defined as:</p> \\[ w_k^{(c)}(x, y) = ReLU\\Big(\\frac{\\partial Y^{(c)}}{\\partial A_k(i, j)}(x, y)\\Big) \\] <p>where \\(A_k(x, y)\\) is the activation of node \\(k\\) in the target layer of the model at position \\((x, y)\\), and \\(Y^{(c)}\\) is the model output score for class \\(c\\) before softmax.</p> Example <pre><code>from torchvision.models import get_model, get_model_weights\nfrom torchcam.methods import LayerCAM\nmodel = get_model(\"resnet18\", weights=get_model_weights(\"resnet18\").DEFAULT).eval()\nwith LayerCAM(model, 'layer4') as cam_extractor:\n    scores = model(input_tensor)\n    cams = cam_extractor(class_idx=100, scores=scores)\n    fused_cam = cam_extractor.fuse_cams(cams)\n</code></pre> PARAMETER DESCRIPTION <code>model</code> <p>input model</p> <p> TYPE: <code>Module</code> </p> <code>target_layer</code> <p>either the target layer itself or its name, or a list of those</p> <p> TYPE: <code>Module | str | list[Module | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_shape</code> <p>shape of the expected input tensor excluding the batch dimension</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>(3, 224, 224)</code> </p> Source code in <code>torchcam/methods/gradient.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    target_layer: nn.Module | str | list[nn.Module | str] | None = None,\n    input_shape: tuple[int, ...] = (3, 224, 224),\n    **kwargs: Any,\n) -&gt; None:\n    super().__init__(model, target_layer, input_shape, **kwargs)\n    # Ensure ReLU is applied before normalization\n    self._relu = True\n    # Model output is used by the extractor\n    self._score_used = True\n    for idx, name in enumerate(self.target_names):\n        # Trick to avoid issues with inplace operations cf. https://github.com/pytorch/pytorch/issues/61519\n        self.hook_handles.append(self.submodule_dict[name].register_forward_hook(partial(self._hook_g, idx=idx)))\n</code></pre>"},{"location":"reference/metrics/","title":"Evaluation metrics","text":"<p>Apart from qualitative visual comparison, it is important to have a refined evaluation metric for class activation maps. This submodule is dedicated to the evaluation of CAM methods.</p>"},{"location":"reference/metrics/#torchcam.metrics.ClassificationMetric","title":"ClassificationMetric","text":"<pre><code>ClassificationMetric(cam_extractor: _CAM, logits_fn: Callable[[Tensor], Tensor] | None = None)\n</code></pre> <p>Implements Average Drop and Increase in Confidence from \"Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks.\".</p> <p>The raw aggregated metric is computed as follows:</p> \\[ \\forall N, H, W \\in \\mathbb{N}, \\forall X \\in \\mathbb{R}^{N \\times 3 \\times H \\times W}, \\forall m \\in \\mathcal{M}, \\forall c \\in \\mathcal{C}, \\\\ AvgDrop_{m, c}(X) = \\frac{1}{N} \\sum\\limits_{i=1}^N f_{m, c}(X_i) \\\\ IncrConf_{m, c}(X) = \\frac{1}{N} \\sum\\limits_{i=1}^N g_{m, c}(X_i) \\] <p>where \\(\\mathcal{C}\\) is the set of class activation generators, \\(\\mathcal{M}\\) is the set of classification models, with the function \\(f_{m, c}\\) defined as:</p> \\[ \\forall x \\in \\mathbb{R}^{3 \\times H \\times W}, f_{m, c}(x) = \\frac{\\max(0, m(x) - m(E_{m, c}(x) * x))}{m(x)} \\] <p>where \\(E_{m, c}(x)\\) is the class activation map of \\(m\\) for input \\(x\\) with method \\(m\\), resized to (H, W),</p> <p>and with the function \\(g_{m, c}\\) defined as:</p> \\[ \\forall x \\in \\mathbb{R}^{3 \\times H \\times W},\\quad g_{m, c}(x) = \\begin{cases}     1 &amp; \\text{if } m(x) &lt; m(E_{m, c}(x) \\cdot x) \\\\     0 &amp; \\text{otherwise} \\end{cases} \\] Example <pre><code>from functools import partial\nfrom torchcam.metrics import ClassificationMetric\nmetric = ClassificationMetric(cam_extractor, partial(torch.softmax, dim=-1))\nmetric.update(input_tensor)\nmetric.summary()\n</code></pre> Source code in <code>torchcam/metrics.py</code> <pre><code>def __init__(\n    self,\n    cam_extractor: _CAM,\n    logits_fn: Callable[[torch.Tensor], torch.Tensor] | None = None,\n) -&gt; None:\n    # This is a typa, I don't know how to rites\n    self.cam_extractor = cam_extractor\n    self.logits_fn = logits_fn\n    self.reset()\n</code></pre>"},{"location":"reference/metrics/#torchcam.metrics.ClassificationMetric.reset","title":"torchcam.metrics.ClassificationMetric.reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the state of the metric.</p> Source code in <code>torchcam/metrics.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset the state of the metric.\"\"\"\n    self.drop = 0.0\n    self.increase = 0.0\n    self.total = 0\n    self.nan_count = 0\n</code></pre>"},{"location":"reference/metrics/#torchcam.metrics.ClassificationMetric.update","title":"torchcam.metrics.ClassificationMetric.update","text":"<pre><code>update(input_tensor: Tensor, class_idx: int | None = None) -&gt; None\n</code></pre> <p>Update the state of the metric with new predictions.</p> PARAMETER DESCRIPTION <code>input_tensor</code> <p>preprocessed input tensor for the model</p> <p> TYPE: <code>Tensor</code> </p> <code>class_idx</code> <p>class index to focus on (default: index of the top predicted class for each sample)</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>torchcam/metrics.py</code> <pre><code>def update(\n    self,\n    input_tensor: torch.Tensor,\n    class_idx: int | None = None,\n) -&gt; None:\n    \"\"\"Update the state of the metric with new predictions.\n\n    Args:\n        input_tensor: preprocessed input tensor for the model\n        class_idx: class index to focus on (default: index of the top predicted class for each sample)\n    \"\"\"\n    self.cam_extractor.model.eval()\n    probs = self._get_probs(input_tensor)\n    # Take the top preds for the cam\n    if isinstance(class_idx, int):\n        cams = self.cam_extractor(class_idx, probs)\n        cam = self.cam_extractor.fuse_cams(cams)\n        probs = probs[:, class_idx]\n    else:\n        preds = probs.argmax(dim=-1)\n        cams = self.cam_extractor(preds.cpu().numpy().tolist(), probs)\n        cam = self.cam_extractor.fuse_cams(cams)\n        probs = probs.gather(1, preds.unsqueeze(1)).squeeze(1)\n    self.cam_extractor.disable_hooks()\n    # Safeguard: skip NaNs\n    discard = torch.isnan(cam).reshape(input_tensor.shape[0], -1).any(dim=-1)\n    cam = cam[~discard, ...]\n    probs = probs[~discard]\n    if class_idx is None:\n        preds = preds[~discard]\n    input_tensor = input_tensor[~discard]\n    # Resize the CAM\n    cam = torch.nn.functional.interpolate(cam.unsqueeze(1), input_tensor.shape[-2:], mode=\"bilinear\")\n    # Create the explanation map &amp; get the new probs\n    with torch.inference_mode():\n        masked_probs = self._get_probs(cam * input_tensor)\n    masked_probs = (\n        masked_probs[:, class_idx]\n        if isinstance(class_idx, int)\n        else masked_probs.gather(1, preds.unsqueeze(1)).squeeze(1)\n    )\n    # Drop (avoid division by zero)\n    drop = torch.relu(probs - masked_probs).div(probs + 1e-7)\n\n    # Increase\n    increase = probs &lt; masked_probs\n\n    self.cam_extractor.enable_hooks()\n\n    self.drop += drop.sum().item()\n    self.increase += increase.sum().item()\n    self.total += cam.shape[0]\n    self.nan_count += discard.sum().item()\n</code></pre>"},{"location":"reference/metrics/#torchcam.metrics.ClassificationMetric.summary","title":"torchcam.metrics.ClassificationMetric.summary","text":"<pre><code>summary() -&gt; dict[str, float]\n</code></pre> <p>Computes the aggregated metrics.</p> RETURNS DESCRIPTION <code>dict[str, float]</code> <p>a dictionary with the average drop and the increase in confidence</p> RAISES DESCRIPTION <code>AssertionError</code> <p>if the metric has not been updated</p> Source code in <code>torchcam/metrics.py</code> <pre><code>def summary(self) -&gt; dict[str, float]:\n    \"\"\"Computes the aggregated metrics.\n\n    Returns:\n        a dictionary with the average drop and the increase in confidence\n\n    Raises:\n        AssertionError: if the metric has not been updated\n    \"\"\"\n    if self.total == 0:\n        raise AssertionError(\"you need to update the metric before getting the summary\")\n\n    return {\n        \"avg_drop\": self.drop / self.total,\n        \"conf_increase\": self.increase / self.total,\n    }\n</code></pre>"},{"location":"reference/utils/","title":"Utilities","text":""},{"location":"reference/utils/#torchcam.utils.overlay_mask","title":"overlay_mask","text":"<pre><code>overlay_mask(img: Image, mask: Image, colormap: Colormap | str = 'jet', alpha: float = 0.7) -&gt; Image\n</code></pre> <p>Overlay a colormapped mask on a background image.</p> Example <pre><code>from PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchcam.utils import overlay_mask\nimg = ...\ncam = ...\noverlay = overlay_mask(img, cam)\n</code></pre> PARAMETER DESCRIPTION <code>img</code> <p>background image</p> <p> TYPE: <code>Image</code> </p> <code>mask</code> <p>mask to be overlayed in grayscale</p> <p> TYPE: <code>Image</code> </p> <code>colormap</code> <p>colormap to be applied on the mask</p> <p> TYPE: <code>Colormap | str</code> DEFAULT: <code>'jet'</code> </p> <code>alpha</code> <p>transparency of the background image</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>overlayed image</p> RAISES DESCRIPTION <code>TypeError</code> <p>when the arguments have invalid types</p> <code>ValueError</code> <p>when the alpha argument has an incorrect value</p> Source code in <code>torchcam/utils.py</code> <pre><code>def overlay_mask(img: Image, mask: Image, colormap: Colormap | str = \"jet\", alpha: float = 0.7) -&gt; Image:\n    \"\"\"Overlay a colormapped mask on a background image.\n\n    Example:\n        ```python\n        from PIL import Image\n        import matplotlib.pyplot as plt\n        from torchcam.utils import overlay_mask\n        img = ...\n        cam = ...\n        overlay = overlay_mask(img, cam)\n        ```\n\n    Args:\n        img: background image\n        mask: mask to be overlayed in grayscale\n        colormap: colormap to be applied on the mask\n        alpha: transparency of the background image\n\n    Returns:\n        overlayed image\n\n    Raises:\n        TypeError: when the arguments have invalid types\n        ValueError: when the alpha argument has an incorrect value\n    \"\"\"\n    if not isinstance(img, Image) or not isinstance(mask, Image):\n        raise TypeError(\"img and mask arguments need to be PIL.Image\")\n\n    if not isinstance(alpha, float) or alpha &lt; 0 or alpha &gt;= 1:\n        raise ValueError(\"alpha argument is expected to be of type float between 0 and 1\")\n\n    cmap = get_cmap(colormap)\n    # Resize mask and apply colormap\n    overlay = mask.resize(img.size, resample=Resampling.BICUBIC)\n    overlay = (255 * cmap(np.asarray(overlay) ** 2)[:, :, :3]).astype(np.uint8)\n    # Overlay the image with the mask\n    return fromarray((alpha * np.asarray(img) + (1 - alpha) * cast(np.ndarray, overlay)).astype(np.uint8))\n</code></pre>"}]}